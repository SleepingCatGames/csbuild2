# Copyright (C) 2013 Jaedyn K. Draper
#
# Permission is hereby granted, free of charge, to any person obtaining
# a copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""
.. package:: _build
	:synopsis: Logic related to actually running a build
"""

# Import this stuff to appease pylint
from __future__ import unicode_literals, division, print_function

import csbuild
import argparse
import os
import sys
import imp
import math
import multiprocessing
import time
import encodings
import importlib
import pkgutil

from . import project_plan, project, input_file
from .. import log
from .._utils import system, shared_globals, thread_pool, terminfo, ordered_set, FormatTime
from .._utils.decorators import TypeChecked
from .._utils.string_abc import String

if sys.version_info[0] >= 3:
	_typeType = type
	_classType = type
	import queue
else:
	import Queue as queue
	import types
	# pylint: disable=invalid-name
	_typeType = types.TypeType
	_classType = types.ClassType

class _dummy(object):
	def __setattr__(self, key, value):
		pass
	def __getattribute__(self, item):
		return ""

class MultiBreak(Exception):
	"""
	Simple exception type to quickly break out of deeply nested loops.
	"""
	pass

_runningBuilds = 0

@TypeChecked(
	pool=thread_pool.ThreadPool,
	projectList=list,
	buildProject=project.Project,
	toolUsed=(_classType, _typeType),
	inputExtension=(String, type(None)),
	inputFiles=(list, ordered_set.OrderedSet),
	outputFiles=(String, tuple)
)
def _buildFinished(pool, projectList, buildProject, toolUsed, inputExtension, inputFiles, outputFiles):
	"""
	Build has finished, enqueue another one.

	:param pool: thread pool
	:type pool: thread_pool.ThreadPool
	:param projectList: list of all projects
	:type projectList: list[project.Project]
	:param buildProject: project
	:type buildProject: project.Project
	:param toolUsed: tool used to build it
	:type toolUsed: type
	:param inputExtension: Extension taken as input
	:type inputExtension: str, bytes
	:param inputFiles: inputs used for this build
	:type inputFiles: list[input_file.InputFile]
	:param outputFiles: output generated by the build
	:type outputFiles: tuple, str, bytes
	"""
	if not isinstance(outputFiles, tuple):
		outputFiles = (outputFiles, )
	for outputFile in outputFiles:
		buildProject.AddArtifact(outputFile)
		global _runningBuilds
		_runningBuilds -= 1
		log.Info("Finished building {} => {}", [os.path.basename(f.filename) for f in inputFiles], os.path.basename(outputFile))

		outputExtension = os.path.splitext(outputFile)[1]
		if inputExtension == outputExtension:
			newInput = input_file.InputFile(outputFile, inputFiles)
		else:
			newInput = input_file.InputFile(outputFile)
		buildProject.inputFiles.setdefault(outputExtension, ordered_set.OrderedSet()).add(newInput)

		buildProject.toolchain.ReleaseReachability(toolUsed)

		# Enqueue this file immediately in any tools that take it as a single input, unless they're marked to delay.
		tools = buildProject.toolchain.GetToolsFor(outputExtension)
		for tool in tools:
			try:
				for dependProject in buildProject.dependencies:
					for dependency in tool.waitForDependentExtensions:
						if dependProject.toolchain.IsOutputActive(dependency):
							raise MultiBreak()

				for dependency in tool.dependencies:
					if buildProject.toolchain.IsOutputActive(dependency):
						raise MultiBreak()
			except MultiBreak:
				continue

			if newInput.WasToolUsed(tool):
				continue

			buildProject.toolchain.CreateReachability(tool)
			newInput.AddUsedTool(tool)
			_runningBuilds += 1
			log.Info("Enqueuing build for {}", newInput)
			pool.AddTask(
				(_logThenRun, tool.Run, tool, buildProject.toolchain, buildProject, newInput),
				(_buildFinished, pool, projectList, buildProject, tool, outputExtension, [newInput])
			)

		log.Info("Checking if {} is still active... {}", outputExtension, "yes" if buildProject.toolchain.IsOutputActive(outputExtension) else "no")

		# If this was the last file being built of its extension, check whether we can pass it and maybe others to relevant group input tools
		if not buildProject.toolchain.IsOutputActive(outputExtension):
			tools = buildProject.toolchain.GetAllTools()
			for tool in tools:
				if not tool.inputGroups:
					continue

				try:
					log.Info("Checking if we can build {} for tool {}", tool.inputGroups, tool.__name__)
					for dependProject in buildProject.dependencies:
						for dependency in tool.waitForDependentExtensions:
							if dependProject.toolchain.IsOutputActive(dependency):
								raise MultiBreak()
					for dependency in tool.dependencies:
						if buildProject.toolchain.IsOutputActive(dependency):
							raise MultiBreak()
				except MultiBreak:
					continue

				# Check for group inputs that have been freed and queue up if all are free
				log.Info("Collecting files...")
				fileList = ordered_set.OrderedSet()
				try:
					for inputFile in tool.inputGroups:
						log.Info("Checking if all builds for {} are done yet", inputFile)
						if buildProject.toolchain.IsOutputActive(inputFile):
							log.Info("Extension {} is still active, can't build yet.", inputFile)
							raise MultiBreak()
						log.Info("{} is ok to build.", inputFile)
						fileList.update([x for x in buildProject.inputFiles.get(inputFile, []) if not x.WasToolUsed(tool)])
				except MultiBreak:
					continue

				if not fileList:
					continue

				buildProject.toolchain.CreateReachability(tool)

				for inputFile in fileList:
					inputFile.AddUsedTool(tool)

				_runningBuilds += 1
				log.Info("Enqueuing multi-build task for {}", fileList)
				pool.AddTask(
					(_logThenRun, tool.RunGroup, tool, buildProject.toolchain, buildProject, fileList),
					(_buildFinished, pool, projectList, buildProject, tool, None, fileList)
				)

			# Check to see if we've freed up any pending builds in other projects as well
			for proj in projectList:
				tools = proj.toolchain.GetAllTools()
				for tool in tools:
					try:
						for dependProject in proj.dependencies:
							for dependency in tool.waitForDependentExtensions:
								if dependProject.toolchain.IsOutputActive(dependency):
									raise MultiBreak()
						for dependency in tool.dependencies:
							if proj.toolchain.IsOutputActive(dependency):
								raise MultiBreak()
					except MultiBreak:
						continue

					for inputFile in tool.inputFiles:
						for projectInput in [x for x in proj.inputFiles.get(inputFile, []) if not x.WasToolUsed(tool)]:
							proj.toolchain.CreateReachability(tool)
							inputFile.AddUsedTool(tool)
							_runningBuilds += 1
							log.Info("Enqueuing build for {}", projectInput)
							pool.AddTask(
								(_logThenRun, tool.Run, tool, proj.toolchain, proj, projectInput),
								(_buildFinished, pool, projectList, proj, tool, inputFile, [projectInput])
							)

					fileList = ordered_set.OrderedSet()
					try:
						for inputFile in tool.inputGroups:
							if proj.toolchain.IsOutputActive(inputFile):
								raise MultiBreak()
							fileList.update([x for x in proj.inputFiles.get(inputFile, []) if not x.WasToolUsed(tool)])
					except MultiBreak:
						continue

					if not fileList:
						continue

					proj.toolchain.CreateReachability(tool)

					for inputFile in fileList:
						inputFile.AddUsedTool(tool)

					_runningBuilds += 1
					log.Info("Enqueuing multi-build task for {}", fileList)
					pool.AddTask(
						(_logThenRun, tool.RunGroup, tool, proj.toolchain, proj, fileList),
						(_buildFinished, pool, projectList, proj, tool, None, fileList)
					)

	if _runningBuilds == 0:
		# We have no builds running and finishing this build did not spawn a new one
		# Time to exit.
		pool.Stop()


def _logThenRun(function, buildTool, buildToolchain, buildProject, inputFiles):
	log.Build("Processing {}", inputFiles)
	with buildToolchain.Use(buildTool):
		return function(buildToolchain, buildProject, inputFiles)

@TypeChecked(numThreads=int, projectBuildList=list)
def _build(numThreads, projectBuildList):
	"""
	Run a build.

	:param numThreads: Number of threads
	:type numThreads: int
	:param projectBuildList: List of projects
	:type projectBuildList: list[project.Project]
	:return:
	"""
	log.Build("Preparing build...")
	buildStart = time.time()
	global _runningBuilds
	callbackQueue = queue.Queue()
	log.SetCallbackQueue(callbackQueue)
	pool = thread_pool.ThreadPool(numThreads, callbackQueue)
	queuedSomething = False
	for buildProject in projectBuildList:
		for extension, fileList in buildProject.inputFiles.items():
			log.Info("Enqueuing tasks for extension {}", extension)
			tools = buildProject.toolchain.GetToolsFor(extension)
			for tool in tools:
				try:
					# For the first pass, if ANY tool in the toolchain is capable of producing this output
					# anywhere in its path, AND any inputs exist for that tool, we won't queue up a build
					log.Info("Checking if we can build {} for tool {}", extension, tool.__name__)
					for dependProject in buildProject.dependencies:
						for dependency in tool.waitForDependentExtensions:
							for otherTool in dependProject.toolchain.GetAllTools():
								extensionSet = otherTool.inputFiles | otherTool.inputGroups
								hasExtension = False
								for dependentExtension in extensionSet:
									if dependProject.inputFiles.get(dependentExtension):
										hasExtension = True
										break
								if hasExtension and dependProject.toolchain.CanCreateOutput(otherTool, dependency):
									raise MultiBreak()

					for dependency in tool.dependencies:
						for otherTool in buildProject.toolchain.GetAllTools():
							extensionSet = otherTool.inputFiles | otherTool.inputGroups
							hasExtension = False
							for dependentExtension in extensionSet:
								if buildProject.inputFiles.get(dependentExtension):
									hasExtension = True
									break
							if hasExtension and buildProject.toolchain.CanCreateOutput(otherTool, dependency):
								raise MultiBreak()
				except MultiBreak:
					continue

				log.Info("Looking at files {}", fileList)
				for inputFile in fileList:
					log.Info("Enqueuing build for {}", inputFile)
					buildProject.toolchain.CreateReachability(tool)
					inputFile.AddUsedTool(tool)
					_runningBuilds += 1
					pool.AddTask(
						(_logThenRun, tool.Run, tool, buildProject.toolchain, buildProject, inputFile),
						(_buildFinished, pool, projectBuildList, buildProject, tool, extension, [inputFile])
					)
					queuedSomething = True

		tools = buildProject.toolchain.GetAllTools()
		log.Info("Checking for group inputs we can run already")
		for tool in tools:
			if not tool.inputGroups:
				continue

			try:
				log.Info("Checking if we can build {} for tool {}", tool.inputGroups, tool.__name__)
				for dependProject in buildProject.dependencies:
					for dependency in tool.waitForDependentExtensions:
						for otherTool in dependProject.toolchain.GetAllTools():
							extensionSet = otherTool.inputFiles | otherTool.inputGroups
							hasExtension = False
							for extension in extensionSet:
								if dependProject.inputFiles.get(extension):
									hasExtension = True
									break
							if hasExtension and dependProject.toolchain.CanCreateOutput(otherTool, dependency):
								raise MultiBreak()
				for dependency in tool.dependencies:
					for otherTool in buildProject.toolchain.GetAllTools():
						extensionSet = otherTool.inputFiles | otherTool.inputGroups
						hasExtension = False
						for extension in extensionSet:
							if buildProject.inputFiles.get(extension):
								hasExtension = True
								break
						if hasExtension and buildProject.toolchain.CanCreateOutput(otherTool, dependency):
							raise MultiBreak()
			except MultiBreak:
				continue

			log.Info("Collecting files...")
			fileList = ordered_set.OrderedSet()

			try:
				for inputFile in tool.inputGroups:
					log.Info("Checking if all builds for {} are done yet", inputFile)
					if buildProject.toolchain.IsOutputActive(inputFile):
						log.Info("Extension {} is still active, can't build yet.", inputFile)
						raise MultiBreak()
					log.Info("{} is ok to build.", inputFile)
					fileList.update(buildProject.inputFiles.get(inputFile, []))
			except MultiBreak:
				continue

			if not fileList:
				break

			buildProject.toolchain.CreateReachability(tool)

			for inputFile in fileList:
				inputFile.AddUsedTool(tool)

			_runningBuilds += 1
			log.Info("Enqueuing multi-build task for {}", fileList)
			pool.AddTask(
				(_logThenRun, tool.RunGroup, tool, buildProject.toolchain, fileList),
				(_buildFinished, pool, projectBuildList, buildProject, tool, None, fileList)
			)
			queuedSomething = True
	if not queuedSomething:
		log.Build("Nothing to build.")
		return

	log.Info("Starting builds")
	failures = 0
	pool.Start()

	while True:
		callback = callbackQueue.get(block=True)
		if callback is thread_pool.ThreadPool.exitEvent:
			break

		toReraise = None
		try:
			callback()
		except thread_pool.ThreadedTaskException as e:
			_runningBuilds -= 1
			if _runningBuilds == 0:
				# We have no builds running and finishing this build did not spawn a new one
				# Time to exit.
				pool.Stop()
			failures += 1
			try:
				toReraise = e
			except csbuild.BuildFailureException as buildExc:
				log.Error(repr(buildExc))
			except:
				pool.Abort()
				raise
		except:
			pool.Abort()
			raise

		if toReraise is not None:
			toReraise.Reraise()

	for buildProject in projectBuildList:
		if buildProject.toolchain.HasAnyReachability():
			log.Error("Project {} did not finish building (likely a toolchain flow error)", buildProject)
			failures += 1

	log.Build("Build finished. Total build time: {}", FormatTime(time.time() - buildStart))
	system.Exit(failures)

@TypeChecked(projectCleanList=list)
def _clean(projectCleanList):
	"""
	Clean the files built in previous builds.

	:param projectCleanList: List of projects
	:type projectCleanList: list[project.Project]
	:return:
	"""
	for cleanProject in projectCleanList:
		for artifact in cleanProject.lastRunArtifacts:
			log.Build("Removing {}", artifact)
			os.remove(artifact)

def _execfile(filename, glob, loc):
	with open(filename, "r") as f:
		glob["__file__"] = filename
		# pylint: disable=exec-used
		exec(compile(f.read(), filename, "exec"), glob, loc)

def Run():
	"""
	Run the build! This is the main entry point for csbuild.
	"""
	mainFileDir = ""
	mainFile = sys.modules['__main__'].__file__
	scriptFiles = []
	makefileDict = {}

	if mainFile is not None:
		mainFileDir = os.path.abspath(os.path.dirname(mainFile))
		if mainFileDir:
			os.chdir(mainFileDir)
			mainFile = os.path.basename(os.path.abspath(mainFile))
		else:
			mainFileDir = os.path.abspath(os.getcwd())
		scriptFiles.append(os.path.join(mainFileDir, mainFile))
		if "-h" in sys.argv or "--help" in sys.argv:
			_execfile(mainFile, makefileDict, makefileDict)
			shared_globals.runMode = csbuild.RunMode.Help
	else:
		log.Error("csbuild cannot be run from the interactive console.")
		system.Exit(1)

	epilog = "    ------------------------------------------------------------    \n\nProjects available in this makefile (listed in build order):\n\n"

	projtable = [[]]
	i = 1
	j = 0

	maxcols = min(math.floor(len(shared_globals.sortedProjects) / 4), 4)

	for proj in shared_globals.sortedProjects:
		projtable[j].append(proj.name)
		if i < maxcols:
			i += 1
		else:
			projtable.append([])
			i = 1
			j += 1

	if projtable:
		maxlens = [15] * len(projtable[0])
		for col in projtable:
			for subindex, item in enumerate(col):
				maxlens[subindex] = max(maxlens[subindex], len(item))

		for col in projtable:
			for subindex, item in enumerate(col):
				item = col[subindex]
				epilog += "  "
				epilog += item
				for _ in range(maxlens[subindex] - len(item)):
					epilog += " "
				epilog += "  "
			epilog += "\n"

	epilog += "\nTargets available in this makefile:\n\n"

	targtable = [[]]
	i = 1
	j = 0

	maxcols = min(math.floor(len(shared_globals.allTargets) / 4), 4)

	for targ in shared_globals.allTargets:
		targtable[j].append(targ)
		if i < maxcols:
			i += 1
		else:
			targtable.append([])
			i = 1
			j += 1

	if targtable:
		maxlens = [15] * len(targtable[0])
		for col in targtable:
			for subindex, item in enumerate(col):
				maxlens[subindex] = max(maxlens[subindex], len(item))

		for col in targtable:
			for subindex, item in enumerate(col):
				epilog += "  "
				epilog += item
				for _ in range(maxlens[subindex] - len(item)):
					epilog += " "
				epilog += "  "
			epilog += "\n"

	parser = shared_globals.parser = argparse.ArgumentParser(
		prog = mainFile, epilog = epilog, formatter_class = argparse.RawDescriptionHelpFormatter)

	parser.add_argument('--version', action = "store_true", help = "Print version information and exit")

	group = parser.add_mutually_exclusive_group()
	group.add_argument('-t', '--target', action='append', choices=shared_globals.allTargets, help = 'Target(s) for build', default=[])
	group.add_argument('--at', "--all-targets", action = "store_true", help = "Build all targets")

	parser.add_argument("-p", "--project", choices=[x.name for x in shared_globals.sortedProjects],
						action="append", help = "Build only the specified project. May be specified multiple times.")

	group = parser.add_mutually_exclusive_group()
	group.add_argument('-c', '--clean', action = "store_true", help = 'Clean the target build')
	#group.add_argument('--install', action = "store_true", help = 'Install the target build')
	#group.add_argument('--install-headers', action = "store_true", help = 'Install only headers for the target build')
	#group.add_argument('--install-output', action = "store_true", help = 'Install only the output for the target build')
	group.add_argument('-r', '--rebuild', action = "store_true", help = 'Clean the target build and then build it')

	group2 = parser.add_mutually_exclusive_group()
	group2.add_argument('-v', '--verbose', action = "store_const", const = 0, dest = "verbosity",
		help = "Verbose. Enables additional INFO-level logging.", default = 1)
	group2.add_argument('-q', '--quiet', action = "store_const", const = 2, dest = "verbosity",
		help = "Quiet. Disables all logging except for WARN and ERROR.", default = 1)
	group2.add_argument('-qq', '--very-quiet', action = "store_const", const = 3, dest = "verbosity",
		help = "Very quiet. Disables all csb-specific logging.", default = 1)

	parser.add_argument("-j", "--jobs", action = "store", dest = "jobs", type = int, help = "Number of simultaneous build processes")

	#parser.add_argument("-g", "--gui", action = "store_true", dest = "gui", help = "Show GUI while building (experimental)")
	#parser.add_argument("--auto-close-gui", action = "store_true", help = "Automatically close the gui on build success (will stay open on failure)")
	#parser.add_argument("--profile", action="store_true", help="Collect detailed line-by-line profiling information on compile time. --gui option required to see this information.")

	parser.add_argument('--show-commands', help = "Show all commands sent to the system.", action = "store_true")
	parser.add_argument('--force-color', help = "Force color on or off.",
		action = "store", choices = ["on", "off"], default = None, const = "on", nargs = "?")
	parser.add_argument('--force-progress-bar', help = "Force progress bar on or off.",
		action = "store", choices = ["on", "off"], default = None, const = "on", nargs = "?")

	#parser.add_argument('--prefix', help = "install prefix (default /usr/local)", action = "store")
	#parser.add_argument('--libdir', help = "install location for libraries (default {prefix}/lib)", action = "store")
	#parser.add_argument('--incdir', help = "install prefix (default {prefix}/include)", action = "store")

	group = parser.add_mutually_exclusive_group()
	group.add_argument('-o', '--toolchain', help = "Toolchain to use for compiling.",
		choices = shared_globals.allToolchains, default=[], action = "append")
	group.add_argument("--ao", '--all-toolchains', help="Build with all toolchains", action = "store_true")

	group = parser.add_mutually_exclusive_group()

	#for toolchainName, toolchainArchStrings in shared_globals.allToolchainArchStrings.items():
	#	archStringLong = "--" + toolchainArchStrings[0]
	#	archStringShort = "--" + toolchainArchStrings[1]
	#	parser.add_argument(archStringLong, archStringShort, help = "Architecture to compile for the {} toolchain.".format(toolchainName), action = "append")

	group.add_argument("-a", "--architecture", "--arch", choices=shared_globals.allArchitectures, help = 'Architecture to compile for each toolchain.', action = "append")
	group.add_argument("--aa", "--all-architectures", "--all-arch", action = "store_true", help = "Build all architectures supported by this toolchain")

	parser.add_argument("--stop-on-error", help = "Stop compilation after the first error is encountered.", action = "store_true")
	#parser.add_argument('--no-precompile', help = "Disable precompiling globally, affects all projects",
	#	action = "store_true")
	#parser.add_argument('--no-chunks', help = "Disable chunking globally, affects all projects",
	#	action = "store_true")
	#parser.add_argument('--dg', '--dependency-graph', help="Generate dependency graph", action="store_true")
	#parser.add_argument('--with-libs', help="Include linked libraries in dependency graph", action="store_true")

	#parser.add_argument("-d", "--define", help = "Add defines to each project being built.", action = "append")

	# group = parser.add_argument_group("Solution generation", "Commands to generate a solution")
	# group.add_argument('--generate-solution', help = "Generate a solution file for use with the given IDE.",
	# 	choices = _shared_globals.allgenerators.keys(), action = "store")
	# group.add_argument('--solution-path',
	# 	help = "Path to output the solution file (default is ./Solutions/<solutiontype>)", action = "store",
	# 	default = "")
	# group.add_argument('--solution-name', help = "Name of solution output file (default is csbuild)", action = "store",
	# 	default = "csbuild")
	# group.add_argument('--solution-args', help = 'Arguments passed to the build script executed by the solution',
	# 	action = "store", default = "")

	#TODO: Additional args here
	# for chain in _shared_globals.alltoolchains.items():
	# 	chainInst = chain[1]()
	# 	argfuncs = set()
	# 	for tool in chainInst.tools.values():
	# 		if(
	# 			hasattr(tool.__class__, "AdditionalArgs")
	# 			and tool.__class__.AdditionalArgs != toolchain.compilerBase.AdditionalArgs
	# 			and tool.__class__.AdditionalArgs != toolchain.linkerBase.AdditionalArgs
	# 		):
	# 			argfuncs.add(tool.__class__.AdditionalArgs)
	#
	# 	if argfuncs:
	# 		group = parser.add_argument_group("Options for toolchain {}".format(chain[0]))
	# 		for func in argfuncs:
	# 			func(group)
	#
	# for gen in _shared_globals.allgenerators.items():
	# 	if gen[1].AdditionalArgs != project_generator.project_generator.AdditionalArgs:
	# 		group = parser.add_argument_group("Options for solution generator {}".format(gen[0]))
	# 		gen[1].AdditionalArgs(group)
	#
	# if _options:
	# 	group = parser.add_argument_group("Local makefile options")
	# 	for option in _options:
	# 		group.add_argument(*option[0], **option[1])

	args, remainder = parser.parse_known_args()
	args.remainder = remainder

	if args.version:
		shared_globals.runMode = csbuild.RunMode.Version

		print("CSBuild version {}".format(csbuild.__version__))
		print(csbuild.__copyright__)
		print("Code by {}".format(csbuild.__author__))
		print("Additional credits: {}\n".format(", ".join(csbuild.__credits__)))
		print("Maintainer: {} - {}".format(csbuild.__maintainer__, csbuild.__email__))
		return

	shared_globals.verbosity = args.verbosity
	shared_globals.showCommands = args.show_commands

	if args.force_color:
		shared_globals.colorSupported = True
	else:
		shared_globals.colorSupported = terminfo.TermInfo.SupportsColor()

	if args.project:
		shared_globals.projectFilter = set(args.project)

	if args.at:
		targetList = list(shared_globals.allTargets)
	elif args.target:
		targetList = args.target
	else:
		targetList = [project_plan.useDefault]

	if args.aa:
		archList = list(shared_globals.allArchitectures)
	elif args.architecture:
		archList = args.architecture
	else:
		archList = [project_plan.useDefault]

	if args.ao:
		toolchainList = list(shared_globals.allToolchains)
	elif args.toolchain:
		toolchainList = args.toolchain
	else:
		toolchainList = [project_plan.useDefault]

	if not args.jobs:
		args.jobs = multiprocessing.cpu_count()

	projectBuildList = []

	preparationStart = time.time()

	_execfile(mainFile, makefileDict, makefileDict)

	for toolchainName in toolchainList:
		log.Info("Collecting projects for toolchain {}", toolchainName)
		for archName in archList:
			log.Info("-- {}", archName)
			for targetName in targetList:
				log.Info("---- {}", targetList)
				for plan in shared_globals.sortedProjects:
					log.Info("------ {}", plan.name)
					proj = plan.ExecutePlan(toolchainName, archName, targetName)
					shared_globals.projectMap.setdefault(proj.toolchainName, {}) \
						.setdefault(proj.archName, {}). \
						setdefault(proj.targetName, {})[plan.name] = proj
					projectBuildList.append(proj)

	for proj in projectBuildList:
		proj.ResolveDependencies()

	shared_globals.projectBuildList = projectBuildList

	totaltime = time.time() - preparationStart
	log.Build("Build preparation took {}".format(FormatTime(totaltime)))

	def _ignoreError(_):
		pass

	# Encodings are handled by trying to import a module and then failing to encode if they can't
	# Import all encodings and cache before releasing the import lock to make sure this can be done in a safe way
	sys.modules.update(
		{
			name: importlib.import_module('encodings.' + name)
			for loader, name, _ in pkgutil.walk_packages(encodings.__path__, onerror=_ignoreError)
		}
	)

	# Note:
	# The reason for this line of code is that the import lock, in the way that CSBuild operates, prevents
	# us from being able to call subprocess.Popen() or any other process execution function other than os.popen().
	# This exists to prevent multiple threads from importing at the same time, so... Within csbuild, never import
	# within any thread but the main thread. Any import statements used by threads should be in the module those
	# thread objects are defined in so they're completed in full on the main thread before that thread starts.
	#
	# After this point, the LOCK IS RELEASED. Importing is NO LONGER THREAD-SAFE. DON'T DO IT.

	#Past this point, disable importing entirely! Anything not already in the cache will raise an error on import
	class _importBlocker(object):
		"""
		This import hook prevents any module from being imported. Ever.
		"""
		#pylint: disable=invalid-name, unused-argument
		def find_module(self, fullname, path=None):
			"""
			Find the module loader, always returns self so the load_module will be called
			:param fullname: name of module
			:type fullname: str
			:param path: path to look in
			:type path: str
			:return: self
			:rtype: _importBlocker
			"""
			return self

		def load_module(self, name):
			"""
			Always raises import error
			:param name: name of module
			:type name: str
			:raises ImportError: always
			"""
			raise ImportError(
				"All modules must be imported prior to build starting. If you need local imports, "
				"make a global import first, or import in a pre-build step, or in plugin/tool static "
				"init, so that it is cached."
			)

	sys.meta_path.insert(0, _importBlocker())

	if imp.lock_held():
		imp.release_lock()

	if args.clean or args.rebuild:
		_clean(projectBuildList)

	if not args.clean or args.rebuild:
		_build(args.jobs, projectBuildList)
